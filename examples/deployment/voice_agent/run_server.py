# -*- coding: utf-8 -*-
"""A test server"""
import asyncio
import os
from dataclasses import asdict

from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.responses import HTMLResponse

from agentscope.agent import RealtimeAgentBase
from agentscope.realtime import DashScopeRealtimeModel  # GeminiRealtimeModel

# ç®€å•çš„ HTML æµ‹è¯•é¡µé¢
html = """
<!DOCTYPE html>
<html>
<head>
    <title>è¯­éŸ³äº¤æµ Demo</title>
    <meta charset="UTF-8">
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 50px auto;
            padding: 20px;
        }
        #messages {
            border: 1px solid #ccc;
            height: 300px;
            overflow-y: scroll;
            padding: 10px;
            margin: 20px 0;
            background: #f9f9f9;
        }
        input[type="text"] {
            width: 70%;
            padding: 10px;
            margin: 5px;
        }
        button {
            padding: 10px 20px;
            margin: 5px;
            cursor: pointer;
        }
        .message {
            margin: 5px 0;
            padding: 5px;
            background: #fff;
            border-radius: 3px;
            border-left: 3px solid #4CAF50;
        }
        .recording {
            background-color: #ff4444 !important;
            animation: pulse 1s infinite;
        }
        @keyframes pulse {
            0%, 100% { opacity: 1; }
            50% { opacity: 0.5; }
        }
        .controls {
            display: flex;
            flex-wrap: wrap;
            gap: 10px;
            margin: 20px 0;
        }
        .status {
            padding: 10px;
            background: #e3f2fd;
            border-radius: 5px;
            margin: 10px 0;
        }
    </style>
</head>
<body>
    <h1>ğŸ¤ è¯­éŸ³äº¤æµ Demo</h1>

    <div class="status">
        <strong>çŠ¶æ€ï¼š</strong><span id="status">æœªè¿æ¥</span>
    </div>

    <div>
        <input type="text" id="userId" placeholder="è¾“å…¥ä½ çš„åå­—" value="User1" />
    </div>

    <div class="controls">
        <button id="voiceBtn" onclick="toggleVoice()">ğŸ¤ å¼€å§‹è¯­éŸ³</button>
        <button onclick="stopVoice()">â¹ï¸ åœæ­¢è¯­éŸ³</button>
        <button onclick="disconnect()">âŒ æ–­å¼€è¿æ¥</button>
    </div>

    <div>
        <input type="text" id="messageText" placeholder="æˆ–è¾“å…¥æ–‡å­—æ¶ˆæ¯..." />
        <button onclick="sendTextMessage()">ğŸ“¤ å‘é€æ–‡å­—</button>
    </div>

    <div id="messages"></div>

    <script>
        let ws = null;
        let audioContext = null;  // ç”¨äºå½•éŸ³ï¼Œ16kHz
        let playbackAudioContext = null;  // ç”¨äºæ’­æ”¾ï¼Œ24kHz
        let mediaStream = null;
        let audioWorkletNode = null;
        let isRecording = false;
        let audioQueue = [];
        let isPlaying = false;
        let audioPlaybackNode = null;
        let audioPlaybackQueue = [];  // å­˜å‚¨è§£ç åçš„ Float32Array
        let audioPlaybackIndex = 0;

        // ç”¨äºç´¯ç§¯è½¬å½•æ–‡æœ¬
        let currentTranscript = "";
        let currentTranscriptElement = null;

        async function connect() {
            const userId = document.getElementById("userId").value;
            ws = new WebSocket(`ws://localhost:8000/ws/${userId}/session1`);

            ws.onopen = function(event) {
                updateStatus("å·²è¿æ¥");
                addMessage("ç³»ç»Ÿ", "âœ… å·²è¿æ¥åˆ°æœåŠ¡å™¨ï¼Œå¯ä»¥å¼€å§‹è¯­éŸ³å¯¹è¯");
            };

            ws.onmessage = async function(event) {
                try {
                    const data = JSON.parse(event.data);
                    console.log("æ”¶åˆ°æ¶ˆæ¯:", data);

                    if (data.type === "audio_delta") {
                        // æ¥æ”¶éŸ³é¢‘æ•°æ®ï¼ŒåŠ å…¥æ’­æ”¾é˜Ÿåˆ—
                        queueAudioChunk(data.audio);
                    } else if (data.type === "text") {
                        // ç´¯ç§¯è½¬å½•æ–‡æœ¬è€Œä¸æ˜¯åˆ›å»ºæ–°æ¶ˆæ¯
                        appendTranscript("AI", data.text || "");
                    } else if (data.type === "transcript_done") {
                        // å®Œæˆå½“å‰è½¬å½•æ¶ˆæ¯
                        finishTranscript();
                    } else {
                        addMessage("ç³»ç»Ÿ", JSON.stringify(data));
                    }
                } catch (e) {
                    console.error("å¤„ç†æ¶ˆæ¯é”™è¯¯:", e);
                }
            };

            ws.onclose = function(event) {
                updateStatus("å·²æ–­å¼€");
                addMessage("ç³»ç»Ÿ", "âŒ å·²æ–­å¼€è¿æ¥");
                stopVoice();
            };

            ws.onerror = function(error) {
                updateStatus("è¿æ¥é”™è¯¯");
                addMessage("ç³»ç»Ÿ", "âš ï¸ è¿æ¥é”™è¯¯");
            };
        }

        async function toggleVoice() {
            if (!isRecording) {
                await startVoice();
            } else {
                stopVoice();
            }
        }

        async function startVoice() {
            try {
                if (!audioContext) {
                    audioContext = new (
                    window.AudioContext || window.webkitAudioContext)({
                        sampleRate: 16000  // DashScope è¦æ±‚ 16kHz
                    });
                }

                mediaStream = await navigator.mediaDevices.getUserMedia({
                    audio: {
                        echoCancellation: true,
                        noiseSuppression: true,
                        sampleRate: 16000
                    }
                });

                const source = audioContext.createMediaStreamSource(
                mediaStream);

                // ä½¿ç”¨ ScriptProcessorNode å¤„ç†éŸ³é¢‘
                const processor = audioContext.createScriptProcessor(4096,
                1, 1);

                let audioChunkCount = 0;
                processor.onaudioprocess = function(e) {
                    if (!isRecording) return;

                    const inputData = e.inputBuffer.getChannelData(0);
                    const pcmData = convertToPCM16(inputData);
                    const base64Audio = arrayBufferToBase64(pcmData);

                    if (ws && ws.readyState === WebSocket.OPEN) {
                        audioChunkCount++;
                        if (audioChunkCount % 10 === 0) {
                        }
                        ws.send(JSON.stringify({
                            type: "input_audio",
                            audio: base64Audio
                        }));
                    }
                };

                source.connect(processor);
                const dummyGain = audioContext.createGain();
                dummyGain.gain.value = 0;  // é™éŸ³ï¼Œé¿å…åé¦ˆ
                processor.connect(dummyGain);
                dummyGain.connect(audioContext.destination);

                isRecording = true;
                document.getElementById("voiceBtn").classList.add("recording");
                document.getElementById("voiceBtn").innerText = "ğŸ”´ å½•éŸ³ä¸­...";
                updateStatus("å½•éŸ³ä¸­");
                addMessage("ç³»ç»Ÿ", "ğŸ¤ å¼€å§‹å½•éŸ³...");

            } catch (err) {
                console.error("å¯åŠ¨å½•éŸ³å¤±è´¥:", err);
                addMessage("ç³»ç»Ÿ", "âš ï¸ æ— æ³•è®¿é—®éº¦å…‹é£: " + err.message);
            }
        }

        function stopVoice() {
            isRecording = false;

            if (mediaStream) {
                mediaStream.getTracks().forEach(track => track.stop());
                mediaStream = null;
            }

            // é€šçŸ¥æœåŠ¡å™¨å½•éŸ³å·²åœæ­¢
            if (ws && ws.readyState === WebSocket.OPEN) {
                ws.send(JSON.stringify({
                    type: "input_audio_done"
                }));
            }

            document.getElementById("voiceBtn").classList.remove("recording");
            document.getElementById("voiceBtn").innerText = "ğŸ¤ å¼€å§‹è¯­éŸ³";
            updateStatus("å·²è¿æ¥");
            addMessage("ç³»ç»Ÿ", "â¹ï¸ åœæ­¢å½•éŸ³");
        }

        function convertToPCM16(float32Array) {
            const int16Array = new Int16Array(float32Array.length);
            for (let i = 0; i < float32Array.length; i++) {
                const s = Math.max(-1, Math.min(1, float32Array[i]));
                int16Array[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
            }
            return int16Array.buffer;
        }

        function arrayBufferToBase64(buffer) {
            const bytes = new Uint8Array(buffer);
            let binary = '';
            for (let i = 0; i < bytes.byteLength; i++) {
                binary += String.fromCharCode(bytes[i]);
            }
            return btoa(binary);
        }

        function queueAudioChunk(base64Audio) {
            try {
                // è§£ç  base64 éŸ³é¢‘æ•°æ®å¹¶è½¬æ¢ä¸º Float32Array
                const binaryString = atob(base64Audio);
                const bytes = new Uint8Array(binaryString.length);
                for (let i = 0; i < binaryString.length; i++) {
                    bytes[i] = binaryString.charCodeAt(i);
                }

                // è½¬æ¢ä¸º Int16Array (PCM16)ï¼Œç„¶åè½¬ä¸º Float32Array
                const int16Array = new Int16Array(bytes.buffer);
                const float32Array = new Float32Array(int16Array.length);

                for (let i = 0; i < int16Array.length; i++) {
                    float32Array[i] = int16Array[i] / 32768.0;
                }

                // å°†è§£ç åçš„éŸ³é¢‘æ•°æ®åŠ å…¥é˜Ÿåˆ—
                audioPlaybackQueue.push(float32Array);

                // å¦‚æœè¿˜æ²¡æœ‰å¼€å§‹æ’­æ”¾ï¼Œå¯åŠ¨æ’­æ”¾å™¨
                if (!isPlaying) {
                    startAudioPlayback();
                }
            } catch (err) {
                console.error("è§£ç éŸ³é¢‘å—å¤±è´¥:", err);
            }
        }

        function startAudioPlayback() {
            if (isPlaying) return;

            try {
                // ä¸ºæ’­æ”¾åˆ›å»ºç‹¬ç«‹çš„ AudioContextï¼Œä½¿ç”¨ 24kHz é‡‡æ ·ç‡
                if (!playbackAudioContext) {
                    playbackAudioContext = new (window.AudioContext ||
                    window.webkitAudioContext)({
                        sampleRate: 24000  // DashScope è¾“å‡º 24kHz
                    });
                }

                // å¦‚æœ AudioContext è¢«æš‚åœï¼ˆæµè§ˆå™¨ç­–ç•¥ï¼‰ï¼Œæ¢å¤å®ƒ
                if (playbackAudioContext.state === 'suspended') {
                    playbackAudioContext.resume();
                }

                isPlaying = true;
                audioPlaybackIndex = 0;

                // ä½¿ç”¨ ScriptProcessorNode è¿›è¡Œæµå¼æ’­æ”¾
                const bufferSize = 4096;
                const processor =
                playbackAudioContext.createScriptProcessor(bufferSize, 0, 1);

                processor.onaudioprocess = function(e) {
                    const output = e.outputBuffer.getChannelData(0);
                    const samplesNeeded = output.length;
                    let samplesWritten = 0;

                    // ä»é˜Ÿåˆ—ä¸­è·å–éŸ³é¢‘æ•°æ®å¹¶å¡«å……è¾“å‡ºç¼“å†²åŒº
                    while (samplesWritten < samplesNeeded &&
                    audioPlaybackQueue.length > 0) {
                        const chunk = audioPlaybackQueue[0];

                        // è®¡ç®—éœ€è¦ä»å½“å‰å—ä¸­è¯»å–çš„æ ·æœ¬æ•°
                        const samplesToRead = Math.min(
                            samplesNeeded - samplesWritten,
                            chunk.length - audioPlaybackIndex
                        );

                        // ç›´æ¥å¤åˆ¶ Float32 æ•°æ®åˆ°è¾“å‡º
                        for (let i = 0; i < samplesToRead; i++) {
                            output[samplesWritten + i] = chunk[
                            audioPlaybackIndex + i];
                        }

                        samplesWritten += samplesToRead;
                        audioPlaybackIndex += samplesToRead;

                        // å¦‚æœå½“å‰å—å·²è¯»å®Œï¼Œç§»é™¤å®ƒå¹¶é‡ç½®ç´¢å¼•
                        if (audioPlaybackIndex >= chunk.length) {
                            audioPlaybackQueue.shift();
                            audioPlaybackIndex = 0;
                        }
                    }

                    // å¦‚æœé˜Ÿåˆ—ä¸ºç©ºä¸”æ²¡æœ‰æ›´å¤šæ•°æ®ï¼Œå¡«å……é™éŸ³
                    if (samplesWritten < samplesNeeded) {
                        for (let i = samplesWritten; i < samplesNeeded; i++) {
                            output[i] = 0;
                        }

                        // å¦‚æœé˜Ÿåˆ—æŒç»­ä¸ºç©ºä¸€æ®µæ—¶é—´ï¼Œåœæ­¢æ’­æ”¾
                        if (audioPlaybackQueue.length === 0) {
                            setTimeout(() => {
                                if (audioPlaybackQueue.length === 0) {
                                    stopAudioPlayback();
                                }
                            }, 100);
                        }
                    }
                };

                processor.connect(playbackAudioContext.destination);
                audioPlaybackNode = processor;

            } catch (err) {
                console.error("å¯åŠ¨éŸ³é¢‘æ’­æ”¾å¤±è´¥:", err);
                isPlaying = false;
            }
        }

        function stopAudioPlayback() {
            if (audioPlaybackNode) {
                audioPlaybackNode.disconnect();
                audioPlaybackNode = null;
            }
            isPlaying = false;
            audioPlaybackQueue = [];
            audioPlaybackIndex = 0;
        }

        function sendTextMessage() {
            const input = document.getElementById("messageText");
            if (ws && input.value) {
                ws.send(JSON.stringify({
                    type: "input_text",
                    text: input.value
                }));
                addMessage("ä½ ", input.value);
                input.value = '';
            }
        }

        function disconnect() {
            stopVoice();
            stopAudioPlayback();
            if (ws) {
                ws.close();
            }
        }

        function updateStatus(text) {
            document.getElementById("status").innerText = text;
        }

        function addMessage(sender, message) {
            const messagesDiv = document.getElementById("messages");
            const messageDiv = document.createElement("div");
            messageDiv.className = "message";
            const time = new Date().toLocaleTimeString();
            messageDiv.innerHTML = `<strong>[${time}] ${sender}:</strong>
            ${message}`;
            messagesDiv.appendChild(messageDiv);
            messagesDiv.scrollTop = messagesDiv.scrollHeight;
        }

        function appendTranscript(sender, text) {
            const messagesDiv = document.getElementById("messages");

            // å¦‚æœè¿˜æ²¡æœ‰å½“å‰æ¶ˆæ¯å…ƒç´ ï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„
            if (!currentTranscriptElement) {
                currentTranscript = "";
                currentTranscriptElement = document.createElement("div");
                currentTranscriptElement.className = "message";
                const time = new Date().toLocaleTimeString();
                currentTranscriptElement.innerHTML = `<strong>[${time}] ${
                sender}:</strong> <span class="transcript-content"></span>`;
                messagesDiv.appendChild(currentTranscriptElement);
            }

            // ç´¯ç§¯æ–‡æœ¬
            currentTranscript += text;

            // æ›´æ–°æ˜¾ç¤ºçš„å†…å®¹
            const contentSpan = currentTranscriptElement.querySelector(
            '.transcript-content');
            if (contentSpan) {
                contentSpan.textContent = currentTranscript;
            }

            // æ»šåŠ¨åˆ°åº•éƒ¨
            messagesDiv.scrollTop = messagesDiv.scrollHeight;
        }

        function finishTranscript() {
            // å®Œæˆå½“å‰è½¬å½•æ¶ˆæ¯ï¼Œå‡†å¤‡ä¸‹ä¸€æ¡
            currentTranscript = "";
            currentTranscriptElement = null;
        }

        // å›è½¦å‘é€æ¶ˆæ¯
        document.getElementById("messageText").addEventListener("keypress",
         function(event) {
            if (event.key === "Enter") {
                sendTextMessage();
            }
        });

        // é¡µé¢åŠ è½½æ—¶è‡ªåŠ¨è¿æ¥
        window.onload = connect;
    </script>
</body>
</html>
"""

app = FastAPI()


@app.get("/")
async def get() -> HTMLResponse:
    """Serve the HTML test page."""
    return HTMLResponse(html)


# pylint: disable=too-many-statements, too-many-branches, unused-argument
@app.websocket("/ws/{user_id}/{session_id}")
async def single_agent_endpoint(
    websocket: WebSocket,
    user_id: str,
    session_id: str,
) -> None:
    """WebSocket endpoint for a single realtime agent."""
    try:
        await websocket.accept()

        # å‘å¾€å‰ç«¯çš„æ¶ˆæ¯é˜Ÿåˆ—
        frontend_queue = asyncio.Queue()

        async def frontend_receive() -> None:
            try:
                while True:
                    msg = await frontend_queue.get()

                    # å°† dataclass å¯¹è±¡è½¬æ¢ä¸ºå­—å…¸
                    if hasattr(msg, "__dataclass_fields__"):
                        msg_dict = asdict(msg)
                    else:
                        msg_dict = msg

                    msg_type = msg_dict.get("type", "unknown")

                    # å¤„ç†éŸ³é¢‘æ•°æ®
                    if msg_type == "response_audio_delta":
                        # è½¬æ¢ä¸ºå‰ç«¯æœŸæœ›çš„æ ¼å¼
                        await websocket.send_json(
                            {
                                "type": "audio_delta",
                                "audio": msg_dict.get("delta", ""),
                            },
                        )
                    elif msg_type == "response_audio_transcript_delta":
                        # å‘é€è½¬å½•æ–‡æœ¬å¢é‡
                        text = msg_dict.get("delta", "")
                        await websocket.send_json(
                            {
                                "type": "text",
                                "text": text,
                            },
                        )
                    elif msg_type == "response_audio_transcript_done":
                        # è½¬å½•å®Œæˆï¼Œé€šçŸ¥å‰ç«¯å¯ä»¥å¼€å§‹æ–°çš„æ¶ˆæ¯
                        await websocket.send_json(
                            {
                                "type": "transcript_done",
                            },
                        )
                    elif msg_type == "input_transcription_done":
                        transcript = msg_dict.get("transcript", "")
                        await websocket.send_json(
                            {
                                "type": "input_transcription",
                                "text": transcript,
                            },
                        )
                    elif msg_type == "error":
                        error_type = msg_dict.get("error_type", "unknown")
                        error_code = msg_dict.get("code", "unknown")
                        error_msg = msg_dict.get("message", "Unknown error")
                        print(
                            f"[ERROR] âš ï¸âš ï¸âš ï¸ Model error: type={error_type}, "
                            f"code={error_code}, message={error_msg}",
                        )
                        await websocket.send_json(
                            {
                                "type": "error",
                                "message": error_msg,
                            },
                        )
                    elif msg_type == "session_created":
                        await websocket.send_json(msg_dict)
                    elif msg_type == "session_ended":
                        await websocket.send_json(msg_dict)
                    else:
                        await websocket.send_json(msg_dict)
            except Exception as e:
                print(f"[ERROR] frontend_receive error: {e}")
                import traceback

                traceback.print_exc()

        asyncio.create_task(frontend_receive())

        api_key = os.getenv("DASHSCOPE_API_KEY")
        if not api_key:
            raise ValueError(
                "DASHSCOPE_API_KEY environment variable is not set!",
            )

        model = DashScopeRealtimeModel(
            model_name="qwen3-omni-flash-realtime",
            api_key=api_key,
        )
        # model = GeminiRealtimeModel(
        #     model_name="gemini-2.5-flash-native-audio-preview-12-2025",
        #     api_key=os.getenv("GEMINI_API_KEY"),
        # )

        agent = RealtimeAgentBase(
            name="Friday",
            sys_prompt="You are a helpful assistant.",
            model=model,
        )

        # or
        # chatroom = ChatRoom(agents=[agent, ...])

        try:
            await agent.start(frontend_queue)
        except Exception as e:
            print(f"[ERROR] Failed to start agent: {e}")
            import traceback

            traceback.print_exc()
            raise
        # or
        # await chatroom.start(frontend_queue)

        try:
            while True:
                data = await websocket.receive_json()
                msg_type = data.get("type", "")

                if msg_type == "input_audio":
                    # å¤„ç†éŸ³é¢‘è¾“å…¥ - é€šè¿‡ agent.model å‘é€
                    audio_base64 = data.get("audio", "")
                    if audio_base64:
                        try:
                            # é€šè¿‡ agent.model å‘é€éŸ³é¢‘æ•°æ®
                            await agent.model.send(
                                {
                                    "type": "audio",
                                    "source": {
                                        "type": "base64",
                                        "media_type": "audio/pcm",
                                        "data": audio_base64,
                                    },
                                },
                            )
                        except Exception as e:
                            print(
                                f"[ERROR] Failed to send audio to model: {e}",
                            )

                elif msg_type == "input_text":
                    # å¤„ç†æ–‡æœ¬è¾“å…¥ - é€šè¿‡ agent.model å‘é€
                    text = data.get("text", "")
                    if text:
                        await agent.model.send(
                            {
                                "type": "text",
                                "text": text,
                            },
                        )

                elif msg_type == "input_audio_done":
                    # éŸ³é¢‘è¾“å…¥å®Œæˆï¼ŒDashScope ä¼šè‡ªåŠ¨æ£€æµ‹å¹¶å¤„ç†
                    pass

                else:
                    # å¤„ç†å…¶ä»–ç±»å‹çš„æ¶ˆæ¯
                    await agent.handle_input(data)
                # or
                # await chatroom.handle_frontend_event(data)

        except WebSocketDisconnect:
            # ä¸éœ€è¦å†æ¬¡å…³é—­ï¼Œè¿æ¥å·²ç»æ–­å¼€
            pass
        finally:
            # æ¸…ç†èµ„æº
            try:
                await agent.stop()
            except Exception as e:
                print(f"[ERROR] Error stopping agent: {e}")

    except Exception as e:
        print(f"[ERROR] WebSocket endpoint error: {e}")
        import traceback

        traceback.print_exc()
        raise


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(
        "run_server:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="debug",
    )
